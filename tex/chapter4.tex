% !TeX root=../main.tex
\chapter{ارزیابی و تحلیل نتایج}
%\thispagestyle{empty} 
\label{chap:results}
\section{مقدمه} 
در این فصل یک کاوش عمیق از مجموعه داده‌ها، مجموعه داده‌های آزمایشی، ابرپارامترها، معیارهای ارزیابی و نتایج مورد استفاده در تحقیق ارائه شده است که با توصیف مجموعه داده‌های اولیه، از جمله ساختار، ویژگی‌ها و ارتباط آنها با حوزه شروع می‌گردد. 

این فصل به تشریح مجموعه داده‌های آزمایشی مورد استفاده برای اعتبارسنجی و مقایسه‌های مورد نیاز ادامه می‌دهد و به دنبال آن یک ارایه با جزییات از ابرپارامترهای انتخاب‌شده برای بهینه‌سازی عملکرد مدل ارائه می‌شود. 

علاوه بر این، معیارهای ارزیابی مورد استفاده جهت تعیین کمیت اثربخشی مدل و تجزیه و تحلیل نتایج تجربی، از جمله آنالیز‌های حساسیت، برای ارزیابی استحکام رویکرد پیشنهادی تعریف شده‌است. این بحث جامع به اعتبار نتایج تحقیق کمک می‌کند.

\section{مجموعه داده‌های مورد استفاده}
در این قسمت دیتاست‌های مورد استفاده در این پژوهش توضیح‌داده شده است.


\subsection{مجموعه داده مووی‌لنز} 
همانطور که در بخش 
\ref{chap:dataset}
گفته شد، از دیتاست ۲۵ میلیونی مووی‌لنز استفاده شده است. این مجموعه داده‌های کاربران و فیلم ها، مانند رتبه‌بندی‌ها، ژانرها، و مُهرهای زمانی را ارائه می‌کند که پایه و اساس نماگر‌سازی کاربر و وظایف پیشنهادی را تشکیل می‌دهند.

 مجموعه داده های موجود در مووی‌لنز به شرح زیر است:
\begin{enumerate}
\item
دیتاست فیلم‌ها:
ویژگی های این دیتاست به شرح زیر است
\begin{itemize}
\item
آیدی: یک شناسه منحصر به فرد برای هر فیلم. 
\item
عنوان: عنوان فیلم به همراه سال اکران آن. 
\item
ژانرها: فهرستی از ژانرهای مرتبط با فیلم.
\end{itemize}

\item
دیتاست امتیازها:
ویژگی های این دیتاست به شرح زیر است
\begin{itemize}
\item
آیدی کاربر: یک شناسه منحصر به فرد برای هر کاربر.
\item
آیدی فیلم: شناسه فیلم رتبه‌بندی‌شده توسط کاربر.
\item
امتیاز: امتیاز ارائه‌شده توسط کاربر، از 0.5 تا 5.0.
\item
مهر زمان: یک مهر زمانی یونیکس از زمانی که امتیاز داده‌شده‌ا‌ست.
\end{itemize}

\item
دیتاست تگ‌ها:
ویژگی‌های این دیتاست به شرح زیر است.
\begin{itemize}
\item
 آیدی کاربر: کاربری که تگ را اضافه کرده‌است.
\item
آیدی فیلم: شناسه فیلم تگ‌شده.
\item
 برچسب: تگ توصیفی اضافه‌شده توسط کاربر.
\item
مهر زمان: یک مهر زمانی یونیکس از زمانی که برچسب اضافه شده‌است.
\end{itemize}
\end{enumerate}

همچنین با استفاده از دیتاست‌های فوق، یک مجموعه داده تلفیقی از پیش پردازش شده MovieRatingTag ایجاد شده‌است.

 ویژگی‌های این مجموعه‌داده به صورت زیر است.
\begin{itemize}
\item
آیدی کاربر: شناسه کاربر.
\item
 آیدی فیلم: شناسه فیلم.
\item
 امتیاز: امتیاز کاربر برای فیلم.
\item
 عنوان: عنوان فیلم.
\item
 ژانرها: ژانرهای مرتبط.
\item
 برچسب: برچسب داده‌شده توسط کاربر، در صورت وجود.
\end{itemize}



\subsection{جمع آوری و آماده سازی داده ها}

با توجه به آن‌که جهت آموزش یک مدل زبانی بزرگ جهت ایجاد یک سیستم گفت‌وگو نیازمند مجموعه داده ای با فرمت سوالات پرسیده و جواب‌های داده‌شده است، یک مجموعه داده به فرمت فوق ایجاد کردیم.

این مجموعه‌داده برگرفته از مووی‌لنز، مکالمه طبیعی را با ایجاد جفت پرسش-پاسخ شبیه سازی می‌کند. این مجموعه داده برای تکرارپذیری و استفاده سایرین در سایت 
\href{https://huggingface.co/datasets/Arefyzd8/MoiveLensDiscussionRecommendationsPrompts}{هاگینگ فیس}
 منتشر شده‌است. 

\label{chap:dataset}
هدف این است که داده‌های خام از منابع مختلف، از جمله مجموعه‌داده 
مووی لنز%
\LTRfootnote{MovieLens}
، به داده‌های گفتگو‌محور تبدیل شوند که مناسب برای انجام تنظیم سریع یا یادگیری درون‌متنی%
\LTRfootnote{In-Context Learning}
 هستند.
\begin{enumerate}
\item
انتخاب و بررسی مجموعه‌داده مووی لنز\\
برای تحلیل اولیه و ایجاد مجموعه‌داده مکالمه‌محور، از 
\href{https://www.kaggle.com/datasets/garymk/movielens-25m-dataset}{مجموعه داده مووی لنز 25 میلیون}
 استفاده شده است. این مجموعه داده به طور گسترده در حوزه‌های مختلف از جمله سیستم‌های توصیه‌گر و فیلتر مشارکتی استفاده می‌شود. منبع این مجموعه داده سایت 
\href{https://www.kaggle.com}{کگل}
 است که حاوی بیش از 25 میلیون رتبه‌بندی، 1 میلیون برنامه برچسب و ابرداده برای 62000 فیلم است. همچنین این مجموعه داده بیش از 20 میلیون رتبه بندی فیلم و برچسب گذاری‌های کاربران است که از سال 1995 جمع آوری کرده است. \\


ویژگی‌های کلیدی این مجموعه داده به شرح زیر است.
\begin{itemize}
\item
امتیازات کاربران: داده‌های صریح شامل رتبه‌بندی کاربران برای فیلم‌ها (1 تا 5 ستاره).
\item
تگ‌ها: عبارات یا کلمات کلیدی مرتبط با فیلم که توسط کاربران اعمال می‌شوند.
\item
ژانرها: دسته‌بندی‌های فیلم، از جمله ژانرهای اکشن، کمدی، درام و غیره.
\item
اطلاعات زمانی: مهر زمانی مربوط به تعاملات کاربران با فیلم‌ها.
\end{itemize}

در این پژوهش، از دیتاست مووی لنز به عنوان یک مثال کاربردی استفاده شده است. همانطور که ذکر شد، این دیتاست شامل اطلاعات مربوط به علایق کاربران در حوزه فیلم‌ها و امتیازدهی به آن‌ها است. با این حال، روش پیشنهادی این پژوهش تنها محدود به این حوزه نیست و می‌تواند برای دیتاست‌های دیگر در حوزه‌های مختلف مانند موسیقی، کتاب، محصولات خرید آنلاین، یا حتی خدمات آموزشی تطبیق داده شود. این انعطاف‌پذیری به دلیل طراحی عمومی الگوریتم‌ها و عدم وابستگی به ویژگی‌های خاص دیتاست است.
 
جزییات روش پیشنهادی که به طور کامل در بخش%
\ref{sec:architecture}
 بیان خواهدشد شامل مراحلی است که مستقل از نوع داده‌ها عمل می‌کنند.

به عنوان مثال، در حوزه فیلم‌ها، داده‌های ورودی شامل امتیازهای کاربران به فیلم‌ها و ژانرهای مورد علاقه آن‌ها است. اما این ورودی‌ها می‌توانند به راحتی با داده‌های دیگری مانند امتیازهای کاربران به آهنگ‌ها یا کتاب‌ها جایگزین شوند. در ادامه، با استفاده از دیتاست مووی لنز، نحوه عملکرد این روش را در قالب یک مثال کاربردی شرح می‌دهیم." 

\item
ترکیب و یکپارچه‌سازی مجموعه‌داده
 
برای ایجاد یک مجموعه داده منسجم و مناسب سیستم گفتگو نیاز به داده‌هایی داریم که دارای ماهیت مکالمه‌محور باشند یا به بیان دیگر این داده‌ها باید به صورت جفت مقدارهای پرسه و پاسخ باشند تا بتوان مدل زبانی خود را با استفاده از این داده ها آموزش داد.

در وهله اول جهت ایجاد یک مجموعه داده متشکل از فیلم ها، نظرات و امتیازات کاربران و غیره به صورت یکجا، فایل‌های امتیازات کاربران، تگ‌های آنها که به فیلم‌های مختلف داده‌اند را بر اساس ستون‌های مشترک مانند آیدی فیلم و آیدی کاربران ادغام کرده و یک مجموعه داده شامل 126 هزار کورد ایجاد نمودیم.

خروجی نهایی این مجموعه داده شامل ستون های زیر است:
\begin{itemize}
\item
آیدی کاربر: شناسه یکتای هر کاربر.
\item
آیدی فیلم: شناسه یکتای هر فیلم.
\item
رتبه‌بندی: امتیاز داده‌شده توسط کاربر.
\item
تگ‌ها: برچسب‌های کاربران برای هر فیلم.
\item
ژانرها: دسته‌بندی‌های فیلم.
\end{itemize}

% جدول نمونه برای فیلدهای موجود در مجموعه داده movieRatingTag
\begin{table}[ht]
    \centering
    \caption{نمونه‌ای از داده‌های یکپارچه شده از مجموعه داده مووی لنز}
    \label{tab:movielens-sample}
    \renewcommand{\arraystretch}{1} % Adjust row height
    \begin{tabularx}{\textwidth}{|>
{\centering\arraybackslash}p{0.1\textwidth}|>
{\centering\arraybackslash}p{0.1\textwidth}|>
{\centering\arraybackslash}p{0.1\textwidth}|>
{\centering\arraybackslash}X|>
{\centering\arraybackslash}X|>
{\centering\arraybackslash}X|}
        \hline
        \textbf{userId} &
        \textbf{movieId} &
        \textbf{rating} &
        \textbf{title} &
        \textbf{genres} &  
        \textbf{tags} \\ 
        \hline
        1629 & 2 & 5.3 & Jumanji (1995) & Adventure, Fantasy & time travel \\ 
        \hline
    \end{tabularx}
\end{table}

\item
ایجاد مجموعه داده پیام‌های مکالمه‌محور:

برای آماده‌سازی مجموعه داده به فرم پرسش و پاسخ‌های گفتگو‌محور مراحل زیر انجام شد:
\begin{itemize}
\item
محاسبه شباهت کسینوسی:
\begin{itemize}
\item
تگ‌های فیلم‌ها به صورت ماتریس برداری با استفاده از CountVectorizer تبدیل شدند.
\item
شباهت کسینوسی بین بردارهای تگ محاسبه شد تا فیلم‌های مشابه شناسایی شوند.
\item
فیلم‌هایی با شباهت بالای 90٪ به عنوان موارد پیشنهادی برای همان فیلم در نظر گرفته شدند.
\end{itemize}

\item
فرمت‌دهی داده‌ها به صورت مکالمه‌ای:

از داده‌های شناسایی‌شده، ساختارهایی با پرسش و پاسخ ایجاد شد.\\
نمونه پرسش:
\begin{quote}
\begin{LTR}
Recommend movies similar to Harry Potter and the Philosopher's Stone
\end{LTR}
\end{quote}
نمونه پاسخ:
\begin{quote}
\begin{LTR}
"Harry Potter and the Chamber of Secrets" for its Hogwarts sequel, "The Lord of the Rings: The Fellowship of the Ring" for its fantasy adventure and "Percy Jackson The Olympians: The Lightning Thief" for its mythological adventure
\end{LTR}
\end{quote}
\end{itemize}

\item
محاسبه شباهت کسینوسی:
برای شناسایی شباهت بین فیلم‌ها و ایجاد جفت‌های پرسش و پاسخ مناسب، از شباهت کسینوسی استفاده شد. رابطه شباهت کسینوسی برای دو بردار متنی A و B به صورت رابطه ی %
\ref{eq:cosineSimilarity}
 تعریف می‌شود:

 
\begin{equation}
\label{eq:cosineSimilarity}
\text{شباهت کسینوسی} = \frac{\sum_{i=1}^{n} A_i \cdot B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \cdot \sqrt{\sum_{i=1}^{n} B_i^2}}
\end{equation}


که در آن مقادیر Ai​ و Bi​ مقدار ویژگی i در بردارهای A و B هستند. همچنین مقدار n برابر با تعداد ویژگی‌ها (کلمات کلیدی یا تگ‌های فیلم‌ها در اینجا) هستند.

این رابطه تضمین می‌کند که شباهت بین دو بردار بر اساس جهت آن‌ها و نه اندازه‌شان محاسبه شود. شباهت کسینوسی معمولاً برای داده‌های متنی و برداری در تحلیل‌های پردازش زبان طبیعی و مدل‌های توصیه‌گر به کار می‌رود.

روش پیاده‌سازی این رابطه به صورت فوق است:
\begin{itemize}
\item
تبدیل تگ‌ها به بردارها: با استفاده از تکنیک CountVectorizer، تگ‌های متنی مربوط به فیلم‌ها به بردارهای عددی تبدیل شدند. این تکنیک بسته کلمات%
\LTRfootnote{Bag of Words (BoW)}
 برای نماگر‌سازی متنی استفاده می‌شود.
\item
محاسبه شباهت: شباهت کسینوسی بین بردارهای مربوط به فیلم‌ها محاسبه شد. همانطور که بیان شد، فیلم‌هایی که شباهت آن‌ها بیش از 90٪ بود، به عنوان فیلم‌های مشابه در نظر گرفته شدند.
\end{itemize}

جهانگیر و همکاران%
\cite{singh2020movie}
 با استفاده از تکنیک فوق، الگوریتم‌هایی توسعه داده شده‌اند که از داده‌های مشابه مووی لنز برای شناسایی فیلم‌های مشابه استفاده می‌کنند. برای مثال، با بهینه‌سازی عبارات کلیدی و اضافه کردن وزن به ژانرها، دقت مدل‌ها افزایش یافته است.


\item
گسترش تنوع پرسش و پاسخ:

با استفاده از فیلم مرجع و فیلم هایی که شباهت کسینوسی بالایی نسبت به فیلم فوق داشته اند، مجموعه ای از فیلم ها به همراه فیلم های مشابه با آن ها را در دسترس خواهیم داشت که می تواند در ادامه ایجاد مجموعه داده مکالمه‌محور کمک کند.\\
برای شباهت بیشتر داده‌ها به گفتگوهای روزمره، 50 قالب مختلف برای پرسش و 50 قالب برای پاسخ طراحی شد.

این قالب ها در پیوست آ%
\ref{naturalDiscussionTemplate}
 آورده شده اند.

به عنوان مثال فیلم Hard Die را در نظر بگیرید، یک نمونه پرسه به فرم زیرخواهد بود:
\begin{quote}
\begin{LTR}
Looking for hidden gems like 'Die Hard 1988' in the action genre
\end{LTR}
\end{quote}

برای فیلم های با ژانرهای مختلف یکی از ژانرها به تصادف برای ایجاد مجموعه داده استفاده می‌شود.
همچنین یک نمونه پاسخ کاندید به صورت زیر خواهد بود:
\begin{quote}
\begin{LTR}
Finding more action films similar to ‘Die Hard 1988'. How about: Hellboy 2004
\end{LTR}
\end{quote}


\item
ایجاد مجموعه‌داده نهایی: 

ساختار مجموعه‌داده نهایی به سبک زیرخواهد بود
\begin{itemize}
\item
ورودی: شامل سوالات کاربران در قالب طبیعی
\item
خروجی: لیست فیلم‌های پیشنهادی
\item
نام فیلم: فیلم ذکر‌شده در سوال
\item
سال: سال انتشار فیلم
\item
ژانر: ژانرهای فیلم.

\end{itemize}

با این روش، مجموعه‌داده‌ای شامل 210 هزار ردیف مکالمه‌ای ایجاد شد که برای مراحل بعدی در توسعه سیستم گفتگوی وظیفه‌گرا مورد استفاده قرار گرفت.


\end{enumerate}


ویژگی‌های مجموعه‌داده به شرح زیر است.
\begin{itemize}
\item
ورودی: اعلان ورودی که به سیستم گفتگو داده می‌شود.
\item
 خروجی: پاسخ مورد انتظار تولید‌شده توسط سیستم گفتگو.
\item
 عنوان فیلم: عنوان فیلم مربوط به دیالوگ.
\item
 سال: سال اکران فیلم.
\item
 ژانرها: ژانر(های) مرتبط با فیلم.
\item
امتیاز شباهت: امتیاز شباهت بین پاسخ‌های تولید‌شده و فیلم موجود در ورودی.
\end{itemize}	

یک نمونه از رکوردهای این دیتاست به فرمت زیر است.

 ورودی: من چند فیلم جنایی می خواهم که مانند«باشگاه مشت زنی (1999)» باشد. پیشنهاد شما چیست؟\\
 خروجی: پس از لذت بردن از «باشگاه مشت زنی (1999)»، فیلم‌های جنایی پنهان را کشف کنید. در اینجا چند پیشنهاد برای فیلم وجود دارد: Seven (1995)\\
 نام فیلم: باشگاه مشت زنی (1999)\\
 سال : 1995\\
 ژانر: اکشن | جنایی | درام | هیجان انگیز\\
 امتیاز شباهت: 
\num{0.93}\\


\section{مجموعه داده تست}
مجموعه‌داده مووی‌لنز برای استخراج نماگر‌های کاربر، داده‌های آموزشی و داده‌های ارزیابی استفاده می‌شود.
نماگر‌های کاربران با تجزیه و تحلیل رتبه‌بندی کاربران، ژانرها و برچسب ها ساخته شده است.
\newline
داده‌های آموزشی شامل 80 درصد از دیتاست شخصی‌سازی‌شده کاربر و تعاملات وی است که برای تنظیم دقیق سیستم گفتگو استفاده می‌شود. با استفاده از این دیتاست، برای هرکاربر موجود در دیتاست یک مدل مجزا آموزش داده شده است که در مرحله ارزیابی مورد استفاده قرار گرفته‌ است.
\newline
داده‌های آموزشی شامل 20 درصد از دیتاست شخصی‌سازی‌شده کاربر و تعاملات وی برای ارزیابی استفاده شده است. ده نمونه تصادفی از هر کاربر برای آزمایش استفاده شد که با استفاده از مدل آموزش‌دیده‌شده خاص کاربر، مجموعه‌ای از جواب‌های شخصی‌سازی‌شده کاربر را ایجاد می‌کند که در نهایت پارامترهای ارزیابی بر روی این مجموعه داده‌ها اعمال خواهند شد.



\section{ابرپارامترها}
ابرپارامترها تنظیمات کلیدی در مدل‌های یادگیری ماشینی هستند که به طور قابل‌توجهی بر آموزش و عملکرد تأثیر می‌گذارند. برای این کار، ابرپارامترهای انتخاب‌شده بر روی تنظیم سریع و رفتار تولید مدل، و همچنین پیکربندی آموزش تمرکز دارند. در زیر به تفکیک هر یک از ابرپارامترها و تأثیر آن می‌پردازیم.
\subsection{ابرپارامترهای تنظیم سریع و تولید پاسخ}

\begin{itemize}
\item
max-length

این مقدار حداکثر تعداد کلماتی را که مدل می‌تواند برای یک پاسخ ایجاد کند را تعیین می‌کند که ارزش 500 برای این مقدار در نظر گرفته شده‌است. 

 مقدار بالاتر، پاسخ‌های طولانی‌تر و دقیق را امکان‌پذیر می‌کند، اما زمان محاسبات و استفاده از حافظه را افزایش می‌دهد در حالی که مقادیر پایین‌تر می‌تواند طول پاسخ را محدود کند و به طور بالقوه پاسخ‌های معنی‌دار را کوتاه کند. مقدار انتخاب‌شده پرحرفی و کارایی محاسباتی را متعادل می‌کند.

\item
pad-token-id

شناسه پدتوکن، نشانه مورد استفاده‌ای است که فاصله‌های توالی به طول یکنواخت را مشخص می‌کند. 

اندازه ورودی ثابت و سازگاری با معماری مدل را تضمین می‌کند و از بروز خطا در طول آموزش یا استنتاج جلوگیری می‌کند. به همین دلیل مقدار tokenizer.eos-token-id برای این ابرپارامتر در نظر گرفته شده‌است.

\item
no-repeat-ngram-size

از تکرار هر اِن-گرام (توالی از n کلمه) در طول تولید توسط مدل جلوگیری می‌کند.

افزایش این مقدار تنوع در متن تولید‌شده را تشویق می‌کند، افزونگی را کاهش می‌دهد و منجر به پاسخ‌های طبیعی‌تر و متنوع‌تر می‌شود. برای این مقدار در آموزش مدل مقدار پنج انتخاب شده‌است.
\item
do-sample

نمونه‌ برداری از توزیع احتمال نشانه بعدی را به جای انتخاب همیشه نشانه با بیشترین احتمال (رمزگشایی حریصانه) فعال می‌کند.

 فعال‌کردن این مقدار در مدل تصادفی‌بودن را به پاسخ‌ها اضافه می‌کند و تنوع را افزایش می‌دهد.
 برای تولید خروجی‌های خلاقانه یا کمتر قطعی مفید است.
\item
top-k

 این ابرپارامتر در آموزش مدل تعداد توکن‌های بالقوه بعدی را به k محتمل‌ترین آن‌ها محدود می‌کند. مقدار در نظرگرفته‌شده برای این پارامتر 100 است.

افزایش مقدار این ابرپارامتر تصادفی‌بودن را به ادامه‌های قابل‌قبول‌تر محدود می‌کند. مقادیر بالاتر تنوع را افزایش می‌دهد اما می‌تواند منجر به پاسخ‌های منسجم کمتری شود.
\item
top-p

توکن‌هایی که احتمال تجمعی آن‌ها با تمرکز بر محتمل‌ترین نتایج تا p باشد را انتخاب می‌کند.

مقدار این ابرپارامتر تعادل بین رفتار قطعی و تصادفی را تضمین می‌کند. مقدار کمتر پاسخ‌های منسجم‌تری را در اولویت قرار می‌دهد.
\item
tempreture

این ابرپارامتر تصادفی‌بودن انتخاب نشانه را در طول تولید کنترل می‌کند.

مقادیر نزدیکتر به صفر مدل را قطعی تر می‌کند، در حالی که مقادیر بالاتر تغییرپذیری را ایجاد می‌کند. مقدار انتخاب شده
\num{0.8}
 است که خروجی‌های متنوع و نسبتاً منسجم را تضمین می‌کند.
\item
 target-device

سخت‌افزار مورد استفاده برای محاسبات را مشخص می‌کند. که با توجه به سیستم و سخت افزار  مقدار، "cuda" در صورت موجود‌بودن یا "cpu" در صورت عدم وجود حافظه گرافیکی مجزا انتخاب می‌شوند

 استفاده از جی‌پی‌یو به طور قابل توجهی سرعت آموزش و استنتاج را افزایش می‌دهد. همچنین مقدار سی‌پی‌یو سازگاری را در سیستم های بدون جی‌پی‌یو تضمین می‌کند.
\end{itemize}

\subsection{ابرپارامترهای آموزشی}

\begin{itemize}
\item
learning-rate


این ابرپارامتر اندازه گام را برای به‌روزرسانی پارامترهای مدل در طول آموزش تعیین می‌کند.

 نرخ بالای یادگیری همگرایی را تسریع می‌کند، اما خطر فراتر رفتن از مقادیر بهینه را به‌همراه دارد که منجر به بی‌ثباتی می‌شود.  نرخ یادگیری تنظیم دقیق معمولا کمتر است (به عنوان مثال، 5e-5)، اما تنظیم سریع از مقادیر بالاتر برای تطبیق سریع جاسازی‌ها استفاده می‌کند. به همین دلیل مقدار 
\num{0.03}
 برای این ابرپارامتر در نظر گرفته شده است.
\item
 prompt-tuning-training-epoch

تعداد ایپاک%
\LTRfootnote{epoch}
‌ها‌ی کامل از مجموعه‌داده آموزشی را مشخص می‌کند.

دوره‌های بیشتر انطباق مدل را بهبود می‌بخشد، اما در صورت زیاده‌روی، خطر 
تطبیق بیش از حد%
\LTRfootnote{Overfitting}
 را دارد. مقدار انتخاب‌شده برای آموزش مدل پنچ ایپاک است عمق تمرین و کارایی زمان را متعادل می‌کند.

\item
 auto-find-batch-size

 با مقداردهی به این ابرپارامتر، مدل به صورت پویا بزرگترین اندازه دسته‌ای را که در حافظه جا می‌گیرد، تعیین می‌کند.

 راه‌اندازی آموزش را با جلوگیری از مشکلات سرریز حافظه ساده می‌کند و همچنین استفاده موثر از سخت‌افزار موجود را تضمین می‌کند.
\item
 no-cuda

اطمینان حاصل می‌کند که مدل در صورت وجود از منابع جی‌پی‌یو استفاده می کند.

 سرعت آموزش و توان عملیاتی مدل را بهبود می بخشد و همچنین سازگاری با هر دو سیستم جی‌پی‌یو و سی‌پی‌یو را تضمین می‌کند.
\end{itemize}

\subsection{تعامل ابرپارامترها}
\begin{itemize}
\item
 تعادل تصادفی و انسجام

 ترکیب top-k ،top-p و tempreture مستقیماً بر تنوع و طبیعی‌بودن پاسخ‌های تولید‌شده تأثیر می‌گذارد. ارزش‌های انتخاب شده به نفع خروجی‌های خلاقانه بدون قربانی کردن انسجام است.
\item
 ثبات آموزش مدل

 نرخ یادگیری بالاتر (3e-2) به‌روزرسانی‌های تعبیه‌شده را در طول تنظیم سریع تسریع می‌کند.
\item
 استفاده از منابع

 ویژگی‌هایی مانند auto-find-batch-size و پشتیبانی از جی‌پی‌یو تضمین می‌کند که فرآیند آموزش با منابع موجود سازگار می‌شود و کارایی را به حداکثر می‌رساند.
\end{itemize}

\section{معیارهای ارزیابی}
در این بخش، به معیارهای ازریابی مورد استفاده در ارزیابی سیستم گفتگو وظیفه‌گرا را بررسی می‌کنیم.

هدف از این معیارهای ارزیابی هم کیفیت پاسخ‌های تولید‌شده توسط مدل و هم میزان موفقیت سیستم گفتگو است. هر معیار بر جنبه‌های خاصی از سیستم، مانند موفقیت‌کار، رضایت کاربر، یا کیفیت پاسخ تمرکز می‌کند. در ادامه بحث مفصلی از معیارهای مربوطه، از جمله تعاریف، محاسبات و نقش آنها در ارزیابی ارائه شده است.

\subsection[معیار گیجی]{معیار گیجی\LTRfootnote{Perplexity}}

گیجی معیاری است برای اینکه یک مدل زبان احتمالی چقدر یک نمونه را پیش‌بینی می‌کند. این نشان دهنده عدم قطعیت مدل در تولید توکن بعدی در یک دنباله است.

\begin{equation}
\label{eq:perplexity}
P(W) = 2^{-\frac{1}{N} \sum_{i=1}^{N} \log_2 P(w_i | w_1, w_2, \ldots, w_{i-1})}
\end{equation}

در رابطه
\ref{eq:perplexity}
مقدار N  تعداد کل توکن ها در دنباله است.
همچنین P(wi) احتمالی که توسط مدل به توکن wi اختصاص داده شده است.


 گیجی کمتر نشان می دهد که مدل در پیش بینی نشانه بعدی بهتر است. در سیستم های گفتگو، گیجی روان بودن پاسخ و ارتباط پاسخ ها را ارزیابی می کند.

\subsection[معیار تمایز]{معیار تمایز\LTRfootnote{Distinct}}

مقاله%
\cite{li2015diversity}
به معیارهای ارزیابی تمایز برای ارزیابی تنوع پاسخ‌های تولید‌شده اشاره می‌کند. تمایز-n، تنوع متن تولیدشده را با محاسبه نسبت n-گرام منحصر‌به‌فرد به کل n-گرام در پاسخ‌ها اندازه گیری می‌کند.


معیار تمایز-1: تنوع یونیگرام‌ها (کلمات فردی) در متن تولیدشده را ارزیابی می‌کند.
\begin{LTR}
\begin{equation}
Distinct-1 = \frac{NumberOfUniqueUnigrams}{TotalNumberOfUnigrams}
\end{equation}
\end{LTR}

معیار تمایز-2: تنوع توالی دو کلمه‌ای‌ها در متن تولیدشده را ارزیابی می‌کند.
\begin{LTR}
\begin{equation}
Distinct-2 = \frac{NumberOfUniqueBigrams}{TotalNumberOfBigrams}
\end{equation}
\end{LTR}

 مقادیر بالاتر نشان‌دهنده پاسخ‌های متنوع‌تر و کمتر تکراری است که برای طبیعی‌بودن مکالمه بسیار مهم است.

\subsection[معیار میزان موفقیت]{معیار میزان موفقیت\LTRfootnote{Success Rate}}

میزان موفقیت یک معیار ارزیابی پرکاربرد برای سیستم‌های گفتگو، به ویژه در سیستم‌های گفتگوی وظیفه‌گرا است. این معیار به صورت کلی درصد اهداف موفق کاربر که توسط سیستم به‌دست‌آمده است را اندازه‌گیری می‌کند.

نرخ موفقیت به عنوان نسبت اهداف موفق کاربر به تعداد کل اهداف کاربر محاسبه می‌شود. هدف کاربر در صورتی موفق تلقی می‌شود که سیستم بتواند درخواست کاربر را برآورده کند یا کار را کامل کند%
\cite{sekulic2024reliable} .

در ارزیابی سیستم گفتگو فوق میزان موفقیت معادل آن است که هر چند وقت یکبار سیستم به یک نتیجه مثبت بر اساس بازخورد کاربر می‌رسد.

در این معیار محاسبه میزان موفقیت آسان است و آن‌ را به معیاری مناسب برای ارزیابی تبدیل می‌کند. همچنین درک روشنی از توانایی سیستم برای تحقق اهداف کاربر ارائه می‌دهد و میزان موفقیت امکان مقایسه سیستم‌های گفتگوی مختلف را فراهم می‌کند. 

رابطه محاسبه میزان موفقیت مطابق با 
\ref{eq:SuccessRate}
است.

\begin{LTR}
\begin{equation}
\label{eq:SuccessRate}
SuccessRate = \frac{Number of 'LIKE' Feedbacks}{Total Feedbacks ('LIKE' + 'DISLIKE')}
\end{equation}
\end{LTR}

با استفاده از این معیار می‌توان توانایی سیستم در برآوردن انتظارات کاربر را ارزیابی کرد. همچنین بازخورد خنثی را برای تمرکز بر ترجیحات صریح کاربر در نظر گرفته نمی‌شود.


\subsection[معیار نرخ تکمیل]{معیار نرخ تکمیل\LTRfootnote{Completion Rate}}


نرخ تکمیل یک معیار ارزیابی است که برای ارزیابی عملکرد سیستم‌های گفتگو، به ویژه در سیستم‌های گفتگوی وظیفه‌گرا استفاده می‌شود. درصد مکالماتی که توسط سیستم با موفقیت انجام شده است را اندازه‌گیری می‌کند% 
\cite{sekulic2024reliable}
.

در واقع نرخ تکمیل کار، توانایی سیستم را برای انجام موفقیت‌آمیز یک کار در نوبت گفتگو اندازه‌گیری می‌کند. معیار تکمیل کار در سیستم گفتگو فوق، نشان‌دهنده موفقیت تکلیف‌محور، مانند ارائه توصیه‌های فیلم یا تکمیل هدف گفتگو است %
\cite{xu2024beyond}
.
 
نرخ تکمیل به عنوان نسبت مکالماتی که معیارهای تکمیل را دارند به تعداد کل مکالمات آغاز شده محاسبه می‌شود.

معیارهای تکمیل به شرح زیر خواهد بود:
\begin{enumerate}
\item
پایان معتبر پیام تولید‌شده: 

متن تولید‌شده با علائم نگارشی معتبر (نشان دهنده پاسخ کامل) به پایان می‌‌رسد.
\item
توصیه فیلم: 

پاسخ حاوی توصیه حداقل یک فیلم است. بعد از دریافت گزاره تولیدشده توسط مدل ، فیلم‌های موجود در آن گزاره از آن استخراج  می‌گردد. این معیار خالی بودن یا نبودن این لیست از فیلم‌های پیشنهادی را بررسی می‌کند.
\end{enumerate}

رابطه محاسبه نرخ تکمیل مطابق با رابطه 
\ref{eq:CompletionRate}
است.

\begin{LTR}
\begin{equation}
\label{eq:CompletionRate}
Completion Rate = \frac{Number of Completed Tasks}{Total Number of Tasks}
\end{equation}
\end{LTR}



این معیار درکی واضح از تکمیل کار را ارائه می‌دهد.نرخ تکمیل نشانه واضحی از توانایی سیستم برای تکمیل مکالمات با موفقیت ارائه می‌دهد.
همچنین با در نظرگرفتن هر دو معیار، میزان تکمیل موفقیت جزئی را در نظر می‌گیرد، مانند مکالماتی که پایان معتبری دارند اما توصیه فیلم ندارند.

\subsection[معیار امتیاز تعامل کاربر]{معیار امتیاز تعامل کاربر\LTRfootnote{User engagement score metric}}

امتیاز تعامل کاربر سطح تعامل کاربران با سیستم را اندازه‌گیری می‌کند. این معیار با توجه به نشست‌های کاربر و بازخوردهایی که به سیستم داده است محاسبه می‌شود
\cite{es2023ragas}
.

برای هر کاربر به ازای هر نشست جدیدی که ایجاد می‌کند، تعداد جواب‌های تولیدی مدل که بازخورد مثبت گرفته‌اند را به تعداد کل پیام‌های ردو‌بدل‌شده مدل تقسیم می‌کند.

\begin{flushright}
\begin{equation}
UES = \left( \frac{Total Interactions in the Session}{Number Of 'LIKE' Feedbacks} \right) \times 10
\end{equation}
\end{flushright}

امتیاز تعامل کاربر بالاتر سیستم جذاب‌تری را پیشنهاد می‌کند.

\subsection[مطابقت تنوع نماگر]{مطابقت تنوع نماگر\LTRfootnote{Profile Diversity Match}}

مطابقت تنوع نماگر تنوع ژانرهای تحت پوشش توصیه‌های سیستم را در مقایسه با ترجیحات کاربر اندازه‌گیری می‌کند. این معیار در واقع ارزیابی می‌کند که سیستم تا چه حد از وسعت علایق کاربر به درستی در جواب‌های تولیدی خود استفاده کرده است.
\begin{equation}
PDM = \frac{Number Of Unique Recommended Genres}{Number Of Unique User-Preferred Genres}
\label{eq:PDM}
\end{equation}
   

 در رابطه 
\ref{eq:PDM}
ژانرهای توصیه‌شده منحصر به فرد برای کاربر در واقع  ژانرهای متمایز در توصیه‌های سیستم گفتگو در طول دوره ارزیابی هستند. همچنین ژانرهای منحصربه‌فرد ترجیحی کاربر، ژانرهای متمایز در نماگر کاربر هستند که در ابتدای ایجاد نماگر به وجود آمده‌اند و در طول زمان نیز به‌روز می‌شوند.\\


مقدار مطابقت تنوع نماگر بالا نشان می‌دهد که سیستم طیف گسترده‌ای از ژانرهای ترجیحی کاربر را پوشش می‌دهد و توانایی آن در تنوع بخشیدن به توصیه‌ها را نشان می‌دهد.
\\
مقدار مطابقت تنوع نماگر پایین نشان می‌دهد که توصیه‌ها تکراری هستند یا نمی توانند دامنه علایق کاربر را جلب کنند.

\section{ارزیابی اعمال تغییرات بر روی معیارهای ارزیابی}

در این بخش تأثیر تکنیک‌های نماگر‌سازی کاربر، به عنوان مثال تحلیل معنایی و فیلتر مشارکتی مبتنی بر آیتم، بر عملکرد مدل و رضایت کاربر ارزیابی شده است.

تحلیل معنایی به درک تفاوت‌های احساسات کمک می‌کند. این تحلیل باعث می‌شود پاسخ‌ها با زمینه و نیازهای کاربر سازگارتر باشند. از طرف دیگر، فیلتر مشارکتی توصیه‌های دقیق‌تری ارائه می‌دهد. این فیلتر بر اساس شباهت‌های بین کاربران عمل می‌کند و دقت توصیه‌ها را تضمین می‌کند.
معیارهای قبلی نشان داده‌اند که ترکیب این دو رویکرد می‌تواند تعامل و دقت را بهبود بخشند. این نتایج نقش حیاتی هر دو رویکرد را در بهبود عملکرد سیستم تأکید می‌کند.

در ادامه، چارچوبی برای آزمایش تأثیرات مستقیم نماگر کاربر ارائه شده است. این چارچوب به بررسی فرضیه‌های خاص می‌پردازد. برای مثال، سهم تحلیل معنایی در درک بهتر زمینه و نقش فیلتر مشارکتی در بهبود دقت توصیه‌ها مورد بررسی قرار می‌گیرد.
هدف از این بخش، جداسازی و اندازه‌گیری تأثیر هر بخش سیستم بر نتایج نهایی است. علاوه بر این، تأثیر ترکیبی مؤلفه‌های مختلف نماگر‌سازی نیز بررسی می‌شود. این بررسی شامل کیفیت تولید گفتگو، موفقیت کاربر در رسیدن به هدف، و معیارهای تعامل کاربر است.

جهت انجام مطالعات فرسایشی%
\LTRfootnote{ablation study}
 برروی بخش های مختلف سیستم، باید سیستم را به طور کامل و در چندین مرحله بررسی کنیم.

در هر مرحله کل سیستم بدون یک قسمت از برنامه مورد ارزیابی مجزا قرارداده می‌شود تا نتایج با و بدون وجود آن قسمت باهم مقایسه شوند و میزان تاثیر قسمت فوق در برنامه مشخص گردد. این رویکرد به شناسایی اهمیت هر جزء کمک می کند و بینش هایی را در مورد استحکام و رفتار سیستم کلی ارائه می کند %
\cite{na2024scalable}
.

همچنین معیارهای دیگر نظیر تغییر بیشینه طول رشته تولیدی توسط مدل نیز در این قسمت مورد ارزیابی قرار گرفته‌است.

در ادامه انواع قسمت‌ها از سیستم حذف شده‌اند و نتایج در قالب جدول آورده شده‌است.


مدل پایه، مدل تولید پاسخ بدون اجزای نماگر کاربر اجرا و ارزیابی‌شده است که نشان می‌د‌هد چگونه هر عنصر نماگر به عملکرد سیستم کمک می‌کند. این راه‌اندازی پایه یک سناریوی کنترلی را فراهم کرده‌است که امکان مقایسه مستقیم و ارائه بینش‌هایی در مورد اثربخشی اجزای نماگر در افزایش کیفیت گفتگو و تعامل کاربر را فراهم می‌کند.

شکل%
\ref{fig:fundation}
نشان دهنده پیکربندی مدل است.


\begin{figure}[ht]
	\centerline{\includegraphics[width=0.8\textwidth]{fundation}}
	\caption{پیکربندی مدل}
	\label{fig:fundation}
\end{figure}

آزمایش‌های فرعی جهت ارزیابی قسمت‌های مختلف دخیل در فرآیند نماگر‌سازی کاربر به شرح زیر دسته‌بندی می‌شوند.
\begin{itemize}
\item
بدون ماژول تحلیل معنایی
\item
بدون فیلتر مشارکتی: صرف نظر از محاسبات شباهت و توصیه‌های مستقیم.
\item
ورودی توکن کاهش/افزایش‌یافته: مقایسه کیفیت کار و پیشنهادهای تولیدی سیستم زمانی که بیشینه طول عبارت تولیدی توسط سیستم کاهش یا افزایش باید.
\end{itemize}

در جدول 
\ref{tab:SensitivityAnalysis}
مقایسه تغییر مقادیر معیارهای ارزیابی معرفی‌شده با استفاده از مطالعات فرسایشی بر روی قسمت‌های مختلف سیستم آورده شده است.

% جدول مقایسه معیارهای ارزیابی با استفاده از مطالعات فرسایشی
\input{./tex/sensitivity-analysis-table}




\subsection{تاثیر ماژول نماگر کاربری بر رضایت کاربران}
مشاهدات از مطالعات فرسایشی بخش نماگر سازی کاربربه شرح زیر است.

\subsubsection{ماژول تحلیل معنایی}

همانطور که انتظار می‌رفت، حذف برچسب‌گذاری احساسات باعث کاهش امتیازهای معیارهای ارزیابی مرتبط به نماگر کاربری شده‌است، زیرا نشانه‌های زمینه‌ای خاص ژانر یا برچسب از بین می‌رود.

معیار PDM زمانی که این مؤلفه حذف می‌شود، دقت و اثربخشی نماگر را کاهش می‌دهد. 

\subsubsection{ماژول فیلتر مشارکتی}

با حذف ماژول فیلتر مشارکتی مبتنی بر آیتم کاهش متوسط ​​در میزان موفقیت و تکمیل قابل مشاهده است، زیرا توصیه‌های شخصی‌شده حذف می‌شوند.
همچنین معیار PDM نشان‌دهنده کاهش جزئی به دلیل عدم وجود بینش مشترک بین نماگر کاربر و نتایج تولیدشده توسط مدل است.

\subsubsection{بدون هیچ‌ گونه نماگر کاربری}

بالاترین گیجی و کمترین UES، که منعکس‌کننده شخصی‌سازی و تعامل ضعیف است. معیار PDM در پایین‌ترین حد خود قرار دارد که نشان‌دهنده حداقل مشارکت در نماگر کاربری است.

\subsection{تاثیر کاهش طول بیشینه متن تولیدی توسط مدل}

تخریب قابل توجه در معیارهای تمایز و موفقیت به‌دلیل زمینه محدود برای تولید پاسخ، که ناشی از اتکای مدل به داده‌های ورودی گسترده‌تر و تولید متن طولانی‌تر برای حفظ انسجام و ایجاد پاسخ‌های متنوع است. این نشان می‌دهد که کاهش طول ورودی، توانایی مدل را برای استفاده مؤثر از داده‌های نماگر کاربر و تاریخچه گفتگو، و همچنین ارایه پیشنهاد در متن خروجی که برای تولید پاسخ‌های ظریف و شخصی‌شده حیاتی هستند، محدود می‌کند.


معیار PDM منعکس کننده تأثیر منفی گسترده‌تری بر تصمیمات نماگر است.

% \subsection{تاثیر استفاده از حق فراموشی در کیفیت خروجی مدل}



\subsection{جمع بندی}
ارزیابی های انجام شده نشان می‌دهد که چگونه تکنیک‌های نماگر کاربر به طور مستقیم با برجسته‌کردن تأثیر آنها بر معیارهایی مانند PDM و UES به عملکرد سیستم کمک می‌کند. 

این تجزیه و تحلیل همچنین زمینه‌های بالقوه برای بهینه‌سازی، مانند پالایش اجزای فیلتر مشارکتی یا تجزیه و تحلیل معنایی را روشن می‌کند و در نتیجه راه را برای تعامل کاربر افزایش‌ یافته و سیستم‌های گفتگوی قوی‌تر در آینده هموار می‌کند. ردیابی امتیاز PDM بینش بیشتری در مورد اثربخشی روش های نماگر ارائه می‌دهد. این یافته‌ها بهینه‌سازی بیشتر سیستم گفتگو و بهبود رضایت کاربر را راهنمایی می‌کند.

\section{نتایج نهایی و مقایسه با کارهای موجود}

\subsection{نتایج نهایی}
در این بخش با تاکید بر ایجاد یک سیستم گفتگوی شخصی با استفاده از تکنیک‌های تنظیم سریع و نماگر کاربری، عملکرد سیستم را از طریق معیارهای مختلفی ارزیابی کردیم. که کیفیت پاسخ های تولید‌شده، تعامل سیستم و رضایت کاربر را ارزیابی شده است. 

روش‌شناسی در اینجا شامل استفاده از سیستم‌های گفتگوی وظیفه‌گرا است که با تنظیم سریع بهبود یافته‌اند، جایی که مدل در طول زمان با ترجیحات کاربر سازگار می‌شود. برای ارزیابی عملکرد سیستم، مجموعه‌ای از معیارهای کلیدی را به کار می‌گیریم، از جمله:

نتایج نهایی و عملکرد روش پیشنهادی برای تمامی معیارهای کلیدی مورد ارزیابی در جدول%
\ref{tab:evaluationMetrics}
آمده است.

\begin{table}[ht]
    \caption{معیارهای ارزیابی و مقادیر آنها}
    \label{tab:evaluationMetrics}
    \centering
    \onehalfspacing
    \begin{tabularx}{0.5\textwidth}{|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|}
        \hline
        \rotatebox{0}{معیار} & 
        \rotatebox{0}{مقدار} \\ \hline
        Perplexity              & \num{8.84}         \\ \hline
        1Distinct               & \num{0.26}         \\ \hline
        2Distinct               & \num{0.58}         \\ \hline
        SuccessRate            & \num{91.97\%}       \\ \hline
        CompletionRate         & \num{93.87\%}       \\ \hline
        UES                     & \num{91.97}       \\ \hline
        PDM                     & \num{94.29}       \\ \hline
    \end{tabularx}
\end{table}


\subsection{مقایسه با کارهای موجود}

در این بخش، عملکرد روش پیشنهادی با کارهای موجود مقایسه شده است. از آنجایی که هیچ مرجعی وجود ندارد که شامل تمام معیارهای ارزیابی خاص ما باشد، مقایسه‌ها بر اساس معیارهای مشترک بین مقالات مختلف انجام شده است. به عبارت دیگر، هر مرجع تنها شامل بخشی از معیارهای ارزیابی است و ما عملکرد خود را با هر مرجع در معیارهای مربوطه مقایسه کرده‌ایم. مقایسه فوق در شکل%
\ref{fig:evaluationMetrics}
آمده است که در ادامه به طور جزیی تر مورد بحث قرارگرفته است.

\begin{figure}[ht]
    \centerline{\includegraphics[width=1\textwidth]{evaluationMetrics}}
    \caption{مقایسه مقادیر معیارهای ارزیابی با کارهای موجود}
    \label{fig:evaluationMetrics}
\end{figure}
\begin{enumerate}
\item
گیجی: گیجی  روش پیشنهادی مقدار 
 \num{8.84}
 است که کمتر از مدل بیان‌شده در مقاله %
\cite{madotto2021few}
با مقدار
\num{9.5}
 و همچنین کمتر از مقدار بیان‌شده در مقاله %
\cite{chung2023instructtods}
با مقدار 
\num{36.63}
است، که نشان می‌دهد مدل در مقایسه با مقالات بیان‌شده در پیش‌بینی‌های خود اطمینان بیشتری دارد. این اختلاف می‌تواند به دلیل ماهیت خاص دامنه و بهبودهای انجام شده هنگام تنظیم سریع مدل باشد.

\item
معیار دیستینک-1: امتیاز مدل بیان شده 
 \num{0.26}
 است که با امتیاز گزارش شده در مقاله%
\cite{kasahara2022building}
\num{0.231}
 مطابقت دارد. این نشان می‌دهد که مدل فوق نسبت به تعداد کل تک‌گرم‌ها، یونی‌گرام‌های منحصربه‌فرد بیشتری تولید می‌کند، که نشان‌دهنده تنوع واژگانی بالاتر در سطح یونیگرام است.

\item
معیار دیستینک-2: مقاله 
\cite{kasahara2022building}
در مقایسه با کار مدل بیان شده با امتیاز 
\num{0.58}
 دارای امتیاز کمی بالاتر 
\num{0.595}
است. این نشان می‌دهد که مدل این مقاله نسبت به تعداد کل بای گرام‌ها، بای گرام‌های منحصربه‌فرد بیشتری تولید می‌کند که نشان‌دهنده تنوع واژگانی اندکی بالاتر در سطح بای‌گرام است.

\item
میزان موفقیت: میزان موفقیت در  روش پیشنهادی 
\num{91.79}
 متفاوت از آنچه در InstructTODS ارائه شده است.
مقاله «InstructTODS: مدل‌های زبان بزرگ برای سیستم‌های گفتگوی وظیفه‌گرا انتها به انتها»
\cite{chung2023instructtods}
 نرخ موفقیت را به عنوان معیاری از توانایی سیستم گفتگو برای دستیابی به هدف کاربر تعریف می‌کند. بر این اساس محاسبه می‌شود که آیا سیستم وظیفه درخواست‌شده توسط کاربر را با موفقیت انجام می‌دهد یا خیر. این مقاله مقدار نهایی
\num{78.48}
را برای میزان موفقیت خود ارایه داده است.

همچنین در 
\cite{hu2024dialight}
این مقدار به صورت میانگین برابر با 
\num{85.1}
محاسبه می‌شود. 


برای معیارهایی مانند میزان موفقیت، که در آن آثار مختلف از تعاریف متفاوتی استفاده می‌کنند تفاوت در روش‌های محاسبه باید مورد توجه قرار گیرد، زیرا ما از یک تعریف سفارشی بر اساس تکمیل کار استفاده می‌کنیم، در حالی که InstructTOD ممکن است شامل عوامل اضافی باشد اما تعریف نهایی همه در حیطه به انجام ‌رساندن وظیفه محول‌شده به سیستم گفتگو یکسان است.

در ارزیابی فوق میزان موفقیت را بر اساس تعداد کارهایی که با موفقیت توسط سیستم انجام شده است، با توجه به تشخیص قصد و دقت پاسخ محاسبه می‌کنیم.

\end{enumerate}


\subsection{ارزیابی انسانی}
علاوه بر ارزیابی‌های انجام شده، یک ارزیابی انسان-هوش مصنوعی انجام شده است که در آن ده ارزیاب انسانی با  روش پیشنهادی تعامل داشتند و در مورد جنبه‌های مختلف عملکرد سیستم بازخورد ارائه می‌کردند. در زیر نتایج آماری برای بازخورد انسانی آورده شده است.

ارزیابی انسانی به فرآیند ارزیابی خروجی های سیستم های توصیه از طریق قضاوت مستقیم انسانی اشاره دارد. این رویکرد به‌ویژه برای ارزیابی جنبه‌های کیفی توصیه‌ها، مانند ارتباط، انسجام، شخصی‌سازی و رضایت کاربر، که ممکن است معیارهای خودکار به‌طور کامل نتوانند از آن‌ها استفاده کنند، مهم است. ارزیاب‌های انسانی معمولاً توصیه‌ها را بر اساس معیارهای از پیش تعریف‌شده رتبه‌بندی یا رتبه‌بندی می‌کنند و بینش‌هایی را در مورد اینکه چقدر سیستم با انتظارات و نیازهای کاربر همسو می‌شود، ارائه می‌دهند.

برای مثال، در سیستم‌های توصیه مبتنی بر LLM، ارزیاب‌های انسانی ممکن است ارزیابی کنند که آیا آیتم‌های توصیه‌شده (مانند فیلم‌ها، محصولات یا مقالات) برای کاربر هدف مناسب، متنوع و جذاب هستند یا خیر %
\cite{elangovan2024considers}
.
نتایج ارزیابی‌های انسانی انجام‌شده به همراه ارسال همان سوالات و پاسخ‌ها به مدل هوش مصنوعی جی‌پی‌تی 
\num{3.5}
جهت انجام ارزیابی هوشمند و مقایسه آن در جدول 
\ref{tab:ComparisonGPT35}
آورده شده است.
\begin{table}[ht]
    \caption{مقایسه نتایج ارزیابی انسان و مدل}
    \label{tab:ComparisonGPT35}
    \centering
    \onehalfspacing
    \begin{tabularx}{\textwidth}{|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|>{\centering\arraybackslash}X|}
        \hline
        \rotatebox{0}{معیارها} & 
        \rotatebox{0}{ارزیابی انسان} &         
        \rotatebox{0}{ارزیابی هوش مصنوعی}  \\
        \hline
        \rotatebox{0}{Perplexity} & 
        \num{13.49} &         
        \num{13.09} \\
        \hline
        \rotatebox{0}{1Distinct} & 
        \num{0.29} &         
        \num{0.29} \\
        \hline
        \rotatebox{0}{2Distinct} & 
        \num{0.60} &         
        \num{0.60} \\
        \hline
        \rotatebox{0}{SuccessRate} & 
        \num{84.42} &         
        \num{85.29}\\
        \hline
        \rotatebox{0}{CompletionRate} & 
        \num{94.19} &         
        \num{93.89}  \\
        \hline
        \rotatebox{0}{UES} & 
        \num{79.57} &         
        \num{85.29}  \\
        \hline
    \end{tabularx}
\end{table}

\begin{itemize}
\item
گیجی: ارزیابی انسان برای گیجی تقریباً با نتایج مدل هوش‌مصنوعی یکسان است.
این نشان می‌دهد که پاسخ‌های این مدل برای انسان‌ها به همان اندازه که برای خود سیستم هوش‌مصنوعی گیج‌کننده است.
\item
دیستنیک ۱و ۲: هر دو معیار بین ارزیابی‌های انسان و هوش‌مصنوعی سازگار هستند، که نشان می‌دهد این مدل پاسخ‌های مشابهی را در تعاملات انسانی ایجاد می‌کند که در ارزیابی‌های هوش‌مصنوعی انجام می‌دهد.
\item
میزان موفقیت: میزان موفقیت در ارزیابی انسانی در مقایسه با نتایج مدل هوش‌مصنوعی کمی کمتر است. این اختلاف را می‌توان به ماهیت ذهنی ارزیابی انسانی نسبت داد، جایی که کاربران ممکن است همیشه در مورد تکمیل کامل یک کار به توافق نرسند.
\item
نرخ تکمیل: میزان تکمیل در هر دو ارزیابی تقریباً یکسان است، با یک تفاوت بسیار کوچک، که نشان می‌دهد سیستم در هر دو ارزیابی پاسخ‌های منسجم و کاملی را ارائه می‌دهد.
\item
 امتیاز تعامل کاربر: در ارزیابی انسانی در مقایسه با هوش‌مصنوعی به طور قابل ‌توجهی کمتر است. این اختلاف ممکن است منعکس‌کننده تفاوت در نحوه درک ارزیابی‌های انسانی از کیفیت تعامل در مقابل ارزیابی داخلی مدل هوش‌مصنوعی باشد.
\end{itemize}

بازخورد انسانی نشان می‌دهد که در حالی که مدل هوش‌مصنوعی در ایجاد پاسخ‌ها عملکرد خوبی دارد، تفاوت‌های ظریفی در نحوه درک انسان از اثربخشی سیستم، به ویژه در مورد تعامل کاربر وجود دارد. این بینش، پیشرفت‌های آینده را در افزایش توانایی سیستم برای تعامل مؤثرتر با کاربران راهنمایی می‌کند.

\subsection{رابط‌کاربری برای ارزیابی انسانی}

برای ارزیابی اثربخشی سیستم گفتگوی وظیفه‌گرا و جمع‌آوری بازخورد انسانی، یک رابط کاربری مبتنی بر اپلیکیشن موبایل نیز توسعه داده شد. این رابط کاربری برای تقلید از سناریوهای دنیای واقعی طراحی شده است که در آن کاربران با سیستم‌های توصیه‌ای تعامل دارند و به آنها اجازه می‌دهد:
\begin{itemize}
\item
ثبت ٰژانرهای مورد علاقه: برای همه کاربران به‌ویژه برای کاربران تازه وارد، امکان انتخاب ژانرهای مورد علاقه فراهم است.
\item
 ارسال پرسش‌ها: کاربران عبارت‌های زبان طبیعی را وارد می‌کنند، مانند «آیا می‌توانید یک فیلم اکشن توصیه کنید؟»
\item
 دریافت پاسخ‌ها: سیستم توصیه‌های سریع تنظیم‌شده متناسب با پرس‌و‌جو و نماگر کاربر را ارائه می‌دهد.
\item
 ارائه بازخورد: کاربران می‌توانند با انتخاب یکی از سه گزینه به پاسخ‌ها واکنش نشان دهند: «پسندیدن»، «نپسندیدن» یا «خنثی».
\end{itemize}

هدف اولیه رابط کاربری ارزیابی عملکرد سیستم از نظر موارد زیر بود:
\begin{itemize}
\item
 ارتباط و انسجام: توسط بازخورد کاربر در مورد پاسخ‌های فردی ارزیابی می‌شود.
\item
 تعامل و رضایت: از طریق تعاملات مکرر و ورودی کیفی کاربر اندازه گیری می‌شود.
\item
 تنوع پاسخ: با مشاهده واکنش کاربران به ژانرها یا سبک‌های مختلف پاسخ‌ها ارزیابی می‌شود.
\end{itemize}

این رابط همچنین ثبت تمام تعاملات کاربر را تسهیل کرد و یک مجموعه داده حیاتی را برای ارزیابی انسانی تشکیل داد. این مجموعه، داده برای محاسبه معیارهایی مانند امتیاز تعامل کاربر و میزان موفقیت و اعتبارسنجی معیارهای خودکار استفاده شد.

یک نمونه از این رابط کاربری مطابق با شکل%
\ref{fig:MindMeldUi}
 است.

\begin{figure}[ht]
	\centerline{\includegraphics[width=0.5\textwidth]{prototype}}
	\caption{رابط کاربری طراحی‌شده}
	\label{fig:MindMeldUi}
\end{figure}



\section{نتیجه‌گیری}

در این فصل، به طور سیستماتیک مجموعه داده‌های مورد استفاده برای آموزش و ارزیابی مدل را با تمرکز اولیه روی مجموعه داده‌های مووی‌لنز مورد بررسی قرار گرفت. 

این مجموعه داده‌های غنی و متنوعی از تعاملات کاربران ارائه می‌دهد که مبنایی برای کارهای شخصی‌سازی و توصیه‌ها را تشکیل می‌دهد. ویژگی‌های مجموعه داده‌ها، از جمله رتبه‌بندی‌های کاربر، ژانرهای فیلم، و مُهرهای زمانی را که برای ایجاد نماگر‌های دقیق کاربر ضروری بودند، به تفصیل شرح داده شدند. 

مجموعه داده‌های آزمایشی به دقت تنظیم شدند تا عملکرد مدل را تحت سناریوهای واقعی ارزیابی کنند و از مقایسه منصفانه و قابل اعتماد نتایج اطمینان حاصل کنند. علاوه بر این، این فصل در مورد پیکربندی ابرپارامتر که برای افزایش کارایی مدل، از جمله نرخ یادگیری به خوبی تنظیم شده بودند، توضیح داده‌شد.

نتایج با استفاده از معیارهایی مانند گیجی، تمایز، میزان موفقیت، میزان تکمیل و امتیاز تعامل کاربر و همچنین معیارهای مرتبط به طور مستقیم با نماگر کاربری مانند دقت توصیه شخصی و تطابق تنوع نماگر ارزیابی شدند. 

این معیارها نقاط قوت و محدودیت‌های مدل را برجسته می‌کنند و نمای شفافی از عملکرد آن در ابعاد مختلف ارائه می‌دهند. تجزیه و تحلیل حساسیت بیشتر استحکام مدل را در برابر تغییرات در پارامترهای کلیدی نشان داد و بر سازگاری و قابلیت اطمینان آن در محیط‌های پویا تأکید کرد. 

این فصل چارچوب آزمایش و ارزیابی دقیق را در بر می‌گیرد که از نتیجه‌گیری‌ها و بینش‌های گسترده‌تر ارائه‌شده در این تحقیق پشتیبانی می‌کند.

 روش پیشنهادی در بسیاری از زمینه‌ها خوب عمل می‌کند، اما فضایی برای بهبود، به ویژه در تعامل با کاربر، همانطور که توسط ارزیابی انسان-هوش‌مصنوعی برجسته شده‌است، نشان می‌دهد.



